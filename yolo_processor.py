"""
YOLO Processor - X·ª≠ l√Ω detection cho video stream
Ch·ª©c nƒÉng:
- Load YOLO model
- ƒê·ªçc stream t·ª´ URL
- Detect v√† v·∫Ω bounding boxes
- Stream l·∫°i video ƒë√£ ƒë∆∞·ª£c detect
"""

import cv2
import numpy as np
from ultralytics import YOLO
import threading
import time
from loguru import logger
import torch


class YOLOStreamProcessor:
    """Class x·ª≠ l√Ω video stream v·ªõi YOLO detection"""

    def __init__(self, model_path="./models/yolo_based/customized_yolo11s.pt"):
        """
        Kh·ªüi t·∫°o YOLO processor

        Args:
            model_path: ƒê∆∞·ªùng d·∫´n ƒë·∫øn model YOLO
        """
        self.model_path = model_path
        self.model = None
        self.stream_url = None
        self.cap = None
        self.is_running = False
        self.current_frame = None
        self.lock = threading.Lock()
        self.detection_thread = None

        # C·∫•u h√¨nh detection
        self.conf_threshold = 0.5  # Ng∆∞·ª°ng confidence
        self.frame_skip = 3  # B·ªè qua nhi·ªÅu frame ƒë·ªÉ gi·∫£m lag (tƒÉng t·ª´ 2 l√™n 5)
        self.frame_count = 0

        # L∆∞u tr·ªØ detections cu·ªëi c√πng ƒë·ªÉ v·∫Ω l·∫°i tr√™n m·ªçi frame
        self.last_detections = []  # [(x1, y1, x2, y2, conf, class_name), ...]

        # WebSocket callback ƒë·ªÉ emit frames
        self.frame_callback = None

        # FPS tracking
        self.fps_start_time = None
        self.fps_frame_count = 0
        self.fps_log_interval = 60  # Log FPS m·ªói 60 frames
        self.current_fps = 0.0  # FPS hi·ªán t·∫°i ƒë·ªÉ v·∫Ω l√™n frame

        # GPU info
        self.gpu_info = "CPU"  # M·∫∑c ƒë·ªãnh CPU

        # Load model
        self.load_model()

    def load_model(self):
        """Load YOLO model"""
        try:
            logger.info(f"Loading YOLO model from {self.model_path}")
            self.model = YOLO(self.model_path)

            # Ki·ªÉm tra GPU
            cuda_available = torch.cuda.is_available()
            if cuda_available:
                gpu_name = torch.cuda.get_device_name(0)
                gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3  # GB
                self.gpu_info = f"{gpu_name} ({gpu_memory:.1f}GB)"
                logger.success(f"‚úì YOLO model loaded successfully!")
                logger.success(f"‚úì GPU: {self.gpu_info}")
                logger.success(f"‚úì CUDA Version: {torch.version.cuda}")
            else:
                self.gpu_info = "CPU (No GPU)"
                logger.warning("‚ö† GPU not available - running on CPU")
                logger.warning("‚ö† Performance will be significantly slower")

        except Exception as e:
            logger.error(f"Failed to load YOLO model: {e}")
            raise

    def set_stream_url(self, url):
        """
        Set URL c·ªßa camera stream

        Args:
            url: URL c·ªßa camera stream
        """
        self.stream_url = url
        logger.info(f"Stream URL set to: {url}")

    def set_frame_callback(self, callback):
        """
        Set callback function ƒë·ªÉ emit frames qua WebSocket

        Args:
            callback: Function nh·∫≠n frame_bytes l√†m parameter
        """
        self.frame_callback = callback
        logger.info("Frame callback set for WebSocket streaming")

    def start_processing(self):
        """B·∫Øt ƒë·∫ßu x·ª≠ l√Ω video stream"""
        if self.is_running:
            logger.warning("Processor is already running")
            return

        if not self.stream_url:
            logger.error("Stream URL not set")
            return

        self.is_running = True
        self.detection_thread = threading.Thread(target=self._process_loop, daemon=True)
        self.detection_thread.start()
        logger.info("Started video processing")

    def stop_processing(self):
        """D·ª´ng x·ª≠ l√Ω video stream"""
        self.is_running = False
        if self.cap:
            self.cap.release()
            self.cap = None
        logger.info("Stopped video processing")

    def _process_loop(self):
        """Loop ch√≠nh ƒë·ªÉ x·ª≠ l√Ω video"""
        try:
            # M·ªü video stream
            self.cap = cv2.VideoCapture(self.stream_url)

            if not self.cap.isOpened():
                logger.error(f"Cannot open stream: {self.stream_url}")
                self.is_running = False
                return

            logger.info("Video stream opened successfully")

            # Kh·ªüi t·∫°o FPS tracking
            self.fps_start_time = time.time()
            self.fps_frame_count = 0

            while self.is_running:
                ret, frame = self.cap.read()

                if not ret:
                    logger.warning("Failed to read frame, retrying...")
                    time.sleep(0.1)
                    continue

                # TƒÉng frame counter
                self.frame_count += 1
                self.fps_frame_count += 1

                # T√≠nh FPS hi·ªán t·∫°i
                if self.fps_frame_count > 1:  # Tr√°nh chia cho 0
                    elapsed = time.time() - self.fps_start_time
                    if elapsed > 0:
                        self.current_fps = self.fps_frame_count / elapsed

                # Log FPS ƒë·ªãnh k·ª≥
                if self.fps_frame_count % self.fps_log_interval == 0:
                    elapsed = time.time() - self.fps_start_time
                    fps = self.fps_frame_count / elapsed
                    logger.info(
                        f"üìä FPS: {fps:.2f} | Frames: {self.fps_frame_count} | Detection every {self.frame_skip} frames"
                    )
                    # Reset counter
                    self.fps_start_time = time.time()
                    self.fps_frame_count = 0

                # Ch·ªâ ch·∫°y detection tr√™n m·ªôt s·ªë frame
                if self.frame_count % self.frame_skip == 0:
                    # Ch·∫°y detection v√† c·∫≠p nh·∫≠t last_detections
                    self._detect_and_update(frame)

                # Lu√¥n v·∫Ω bounding boxes (d√πng detection c≈© n·∫øu kh√¥ng ch·∫°y detection m·ªõi)
                processed_frame = self._draw_boxes(frame, self.last_detections)

                # V·∫Ω performance stats l√™n frame
                processed_frame = self._draw_performance_stats(processed_frame)

                # L∆∞u frame ƒë√£ x·ª≠ l√Ω
                with self.lock:
                    self.current_frame = processed_frame

                # Emit frame qua WebSocket callback n·∫øu c√≥
                if self.frame_callback:
                    try:
                        # Encode frame th√†nh JPEG
                        ret, buffer = cv2.imencode(".jpg", processed_frame, [cv2.IMWRITE_JPEG_QUALITY, 85])
                        if ret:
                            frame_bytes = buffer.tobytes()
                            self.frame_callback(frame_bytes)
                    except Exception as e:
                        logger.error(f"Error in frame callback: {e}")

        except Exception as e:
            logger.error(f"Error in process loop: {e}")
        finally:
            if self.cap:
                self.cap.release()

    def _detect_and_update(self, frame):
        """
        Ch·∫°y detection v√† c·∫≠p nh·∫≠t last_detections

        Args:
            frame: Frame t·ª´ video
        """
        try:
            # Ch·∫°y YOLO detection
            results = self.model.predict(frame, conf=self.conf_threshold, verbose=False)

            # C·∫≠p nh·∫≠t detections
            detections = []
            for result in results:
                boxes = result.boxes
                for box in boxes:
                    # L·∫•y th√¥ng tin box
                    x1, y1, x2, y2 = map(int, box.xyxy[0])
                    conf = float(box.conf[0])
                    cls_id = int(box.cls[0])
                    class_name = self.model.names[cls_id]

                    # B·ªè qua class "natural"
                    if class_name == "natural":
                        continue

                    detections.append((x1, y1, x2, y2, conf, class_name))

            # C·∫≠p nh·∫≠t last_detections
            self.last_detections = detections

        except Exception as e:
            logger.error(f"Error in detection: {e}")

    def _draw_boxes(self, frame, detections):
        """
        V·∫Ω bounding boxes l√™n frame

        Args:
            frame: Frame t·ª´ video
            detections: List of (x1, y1, x2, y2, conf, class_name)

        Returns:
            Frame ƒë√£ ƒë∆∞·ª£c v·∫Ω bounding boxes
        """
        annotated_frame = frame.copy()

        for x1, y1, x2, y2, conf, class_name in detections:
            # Ch·ªçn m√†u theo class
            color = self._get_color_for_class(class_name)

            # V·∫Ω bounding box (gi·∫£m ƒë·ªô d√†y t·ª´ 2 xu·ªëng 1)
            cv2.rectangle(annotated_frame, (x1, y1), (x2, y2), color, 1)

            # V·∫Ω label
            label = f"{class_name}: {conf:.2f}"
            label_size, _ = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1)

            # V·∫Ω background cho text
            cv2.rectangle(annotated_frame, (x1, y1 - label_size[1] - 8), (x1 + label_size[0], y1), color, -1)

            # Ch·ªçn m√†u ch·ªØ: ƒëen cho look_away (n·ªÅn v√†ng), tr·∫Øng cho c√°c class kh√°c
            text_color = (0, 0, 0) if class_name == "look_away" else (255, 255, 255)

            # V·∫Ω text (gi·∫£m font size v√† thickness)
            cv2.putText(annotated_frame, label, (x1, y1 - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, text_color, 1)

        return annotated_frame

    def _draw_performance_stats(self, frame):
        """
        V·∫Ω performance stats l√™n frame (FPS, GPU info)

        Args:
            frame: Frame ƒë√£ v·∫Ω bounding boxes

        Returns:
            Frame v·ªõi performance overlay
        """
        h, w = frame.shape[:2]

        # Th√¥ng tin hi·ªÉn th·ªã
        fps_text = f"FPS: {self.current_fps:.1f}"
        gpu_text = f"Device: {self.gpu_info}"
        objects_text = f"Objects: {len(self.last_detections)}"

        # C·∫•u h√¨nh hi·ªÉn th·ªã
        font = cv2.FONT_HERSHEY_SIMPLEX
        font_scale = 0.6
        thickness = 2
        padding = 10
        line_height = 30

        # V·∫Ω n·ªÅn semi-transparent cho stats panel
        overlay = frame.copy()
        panel_height = line_height * 3 + padding * 2
        cv2.rectangle(overlay, (10, 10), (400, 10 + panel_height), (0, 0, 0), -1)
        cv2.addWeighted(overlay, 0.6, frame, 0.4, 0, frame)

        # V·∫Ω text
        y_offset = 10 + padding + 20
        cv2.putText(frame, fps_text, (20, y_offset), font, font_scale, (0, 255, 0), thickness)
        y_offset += line_height
        cv2.putText(frame, gpu_text, (20, y_offset), font, font_scale, (0, 255, 255), thickness)
        y_offset += line_height
        cv2.putText(frame, objects_text, (20, y_offset), font, font_scale, (255, 255, 255), thickness)

        return frame

    def _detect_and_draw(self, frame):
        """
        DEPRECATED: S·ª≠ d·ª•ng _detect_and_update + _draw_boxes thay th·∫ø
        Ch·∫°y detection v√† v·∫Ω bounding boxes

        Args:
            frame: Frame t·ª´ video

        Returns:
            Frame ƒë√£ ƒë∆∞·ª£c v·∫Ω bounding boxes
        """
        try:
            # Ch·∫°y YOLO detection
            results = self.model.predict(frame, conf=self.conf_threshold, verbose=False)

            # V·∫Ω bounding boxes
            annotated_frame = frame.copy()

            for result in results:
                boxes = result.boxes
                for box in boxes:
                    # L·∫•y th√¥ng tin box
                    x1, y1, x2, y2 = map(int, box.xyxy[0])
                    conf = float(box.conf[0])
                    cls_id = int(box.cls[0])
                    class_name = self.model.names[cls_id]

                    # B·ªè qua class "natural" - kh√¥ng v·∫Ω bounding box
                    if class_name == "natural":
                        continue

                    # Ch·ªçn m√†u theo class
                    color = self._get_color_for_class(class_name)

                    # V·∫Ω bounding box
                    cv2.rectangle(annotated_frame, (x1, y1), (x2, y2), color, 2)

                    # V·∫Ω label
                    label = f"{class_name}: {conf:.2f}"
                    label_size, _ = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.6, 2)

                    # V·∫Ω background cho text
                    cv2.rectangle(annotated_frame, (x1, y1 - label_size[1] - 10), (x1 + label_size[0], y1), color, -1)

                    # V·∫Ω text
                    cv2.putText(annotated_frame, label, (x1, y1 - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)

            return annotated_frame

        except Exception as e:
            logger.error(f"Error in detection: {e}")
            return frame

    def _get_color_for_class(self, class_name):
        """
        L·∫•y m√†u cho t·ª´ng lo·∫°i class

        Args:
            class_name: T√™n class

        Returns:
            Tuple m√†u BGR
        """
        color_map = {
            "sleepy_eye": (0, 0, 255),  # ƒê·ªè t∆∞∆°i - nguy hi·ªÉm cao nh·∫•t
            "yawn": (0, 140, 255),  # Cam ƒë·∫≠m - bu·ªìn ng·ªß
            "look_away": (0, 255, 255),  # V√†ng - m·∫•t t·∫≠p trung
            "phone": (255, 0, 255),  # T√≠m/H·ªìng - d√πng ƒëi·ªán tho·∫°i
            "rub_eye": (60, 180, 255),  # Cam nh·∫°t - d·ª•i m·∫Øt
            "natural": (0, 255, 0),  # Xanh l√° - b√¨nh th∆∞·ªùng
        }
        return color_map.get(class_name, (255, 255, 255))  # Tr·∫Øng - m·∫∑c ƒë·ªãnh

    def get_current_frame(self):
        """
        L·∫•y frame hi·ªán t·∫°i ƒë√£ ƒë∆∞·ª£c x·ª≠ l√Ω

        Returns:
            Frame ƒë√£ ƒë∆∞·ª£c detect (numpy array) ho·∫∑c None
        """
        with self.lock:
            if self.current_frame is not None:
                return self.current_frame.copy()
            return None

    def generate_frames(self):
        """
        Generator ƒë·ªÉ stream frames qua HTTP (MJPEG)

        Yields:
            Bytes c·ªßa frame d∆∞·ªõi d·∫°ng JPEG
        """
        while True:
            frame = self.get_current_frame()

            if frame is None:
                time.sleep(0.1)
                continue

            # Encode frame th√†nh JPEG
            ret, buffer = cv2.imencode(".jpg", frame)
            if not ret:
                continue

            frame_bytes = buffer.tobytes()

            # Yield frame theo format MJPEG
            yield (b"--frame\r\n" b"Content-Type: image/jpeg\r\n\r\n" + frame_bytes + b"\r\n")


# Multi-instance management - M·ªói stream_url c√≥ 1 processor ri√™ng
_processor_instances = {}


def get_processor(stream_url):
    """
    L·∫•y ho·∫∑c t·∫°o processor cho stream_url c·ª• th·ªÉ

    Args:
        stream_url: URL c·ªßa stream c·∫ßn detect

    Returns:
        YOLOStreamProcessor instance cho stream ƒë√≥
    """
    global _processor_instances

    if stream_url not in _processor_instances:
        logger.info(f"Creating new YOLO processor for stream: {stream_url}")
        processor = YOLOStreamProcessor()
        processor.set_stream_url(stream_url)
        _processor_instances[stream_url] = processor

    return _processor_instances[stream_url]


def remove_processor(stream_url):
    """
    X√≥a processor cho stream_url c·ª• th·ªÉ

    Args:
        stream_url: URL c·ªßa stream c·∫ßn x√≥a processor
    """
    global _processor_instances

    if stream_url in _processor_instances:
        logger.info(f"Removing YOLO processor for stream: {stream_url}")
        _processor_instances[stream_url].stop_processing()
        del _processor_instances[stream_url]


def get_active_streams():
    """
    L·∫•y danh s√°ch c√°c stream ƒëang active

    Returns:
        List of stream URLs ƒëang ƒë∆∞·ª£c detect
    """
    return [url for url, proc in _processor_instances.items() if proc.is_running]
